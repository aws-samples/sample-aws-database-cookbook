{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aurora Read Replica Auto Scaling Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #f8f9fa; border: 1px solid #e9ecef; border-radius: 8px; padding: 10px; margin: 10px;\">\n",
    "<strong>üìã Workshop Contents</strong>\n",
    "<ul style=\"line-height: 1.2;\">\n",
    "<li><a href=\"#What-We'll-Build\">What We'll Build</a></li>\n",
    "<li><a href=\"#Prerequisites\">Prerequisites</a></li>\n",
    "<li><a href=\"#Step-1:-Configure-Aurora-Auto-Scaling\">Step 1: Configure Aurora Auto Scaling</a></li>\n",
    "<li><a href=\"#Step-2:-Generate-CPU-Intensive-Workload\">Step 2: Generate CPU Intensive Workload</a></li>\n",
    "<li><a href=\"#Step-3:-Monitor-Auto-Scaling-Events\">Step 3: Monitor Auto Scaling Events</a></li>\n",
    "<li><a href=\"#Creating-Read-Replicas\">Creating Read Replicas</a></li>\n",
    "<li><a href=\"#Connection-Routing-and-Load-Balancing\">Connection Routing and Load Balancing</a></li>\n",
    "<li><a href=\"#Additional-Resources\">Additional Resources</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We'll Build\n",
    "\n",
    "This code demonstrates Aurora auto scaling with read replicas by:\n",
    "- Configuring Aurora auto scaling with low CPU thresholds for testing\n",
    "- Generating CPU-intensive workload using pgbench\n",
    "- Monitoring auto scaling events and metrics\n",
    "- Creating Aurora read replicas for load distribution\n",
    "- Showing connection routing between writer and reader endpoints\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this workshop, ensure you have:\n",
    "\n",
    "- ‚úÖ **Jupyter Notebook**: You can launch a [free tier Amazon SageMaker Jupyter Notebook](../../1_Getting_Started_with_AWS/1.4_Setting_up_Your_Cookbook_Environment/README.MD)\n",
    "- ‚úÖ **Aurora PostgreSQL Cluster**: An active Aurora PostgreSQL cluster running in your AWS account\n",
    "  - If you don't have one, follow the setup guide: [Your First Database on AWS](../../2_Your_First_Database_on_AWS/README.MD)\n",
    "- ‚úÖ **Network Connectivity**: Ensure your Jupyter notebook can connect to the Amazon Aurora cluster\n",
    "  - Verify security groups allow connections from the Jupyter notebook, for example, check `telnet <Aurora cluster endpoint> <database port>` from your Jupyter notebook's terminal.\n",
    "  - Database should be accessible from the VPC where the notebook is running.\n",
    "- ‚úÖ **Database Credentials**: Valid username and password for the Aurora cluster\n",
    "- ‚úÖ **IAM Permissions** for:\n",
    "  - Application Auto Scaling (for Aurora auto scaling configuration)\n",
    "  - RDS cluster management and monitoring\n",
    "  - CloudWatch metrics access\n",
    "- ‚úÖ **PostgreSQL Tools**: pgbench installed (included in Amazon SageMaker notebook environment)\n",
    "\n",
    "> üí° **Note**: This workshop demonstrates Aurora auto scaling using pgbench load testing. The low CPU thresholds are for demonstration purposes only.\n",
    "\n",
    "**Important:** Before running this code, update the following variables with your actual values:\n",
    "- `cluster_id`: Your Aurora cluster identifier\n",
    "- `db_host`: Your Aurora cluster endpoint\n",
    "- `db_username`: Database username\n",
    "- `db_password`: Database password\n",
    "- `aws_region`: Your AWS region (e.g., \"us-east-1\")\n",
    "\n",
    "> üîó **Using Previous Aurora Cluster**: If you completed the previous sections, you can reuse the Aurora cluster and credentials from [Your First Database on AWS](../../2_Your_First_Database_on_AWS/README.MD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuration - Update these values\n",
    "cluster_id = \"your-aurora-cluster\"\n",
    "aws_region = \"us-east-1\"\n",
    "replica_class = \"db.r7g.large\"\n",
    "db_host = \"your-cluster-endpoint.cluster-xxx.us-east-1.rds.amazonaws.com\"\n",
    "db_username = \"postgres\"\n",
    "db_password = \"your-password\"\n",
    "db_name = \"postgres\"\n",
    "\n",
    "print(\"üöÄ Aurora Auto Scaling Demo Configuration:\")\n",
    "print(f\"Cluster ID: {cluster_id}\")\n",
    "print(f\"Region: {aws_region}\")\n",
    "print(f\"Instance Class: {replica_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configure Aurora Auto Scaling\n",
    "\n",
    "First, we'll configure Aurora auto scaling with a low CPU threshold (20%) for testing purposes. This will trigger scaling events more easily during our load test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_aurora_autoscaling(cluster_identifier, region='us-east-1'):\n",
    "    \"\"\"Configure Aurora auto scaling with low CPU threshold for testing\"\"\"\n",
    "    autoscaling = boto3.client('application-autoscaling', region_name=region)\n",
    "    cloudwatch = boto3.client('cloudwatch', region_name=region)\n",
    "    \n",
    "    resource_id = f\"cluster:{cluster_identifier}\"\n",
    "    \n",
    "    try:\n",
    "        # Register scalable target\n",
    "        print(\"üìä Registering Aurora cluster as scalable target...\")\n",
    "        autoscaling.register_scalable_target(\n",
    "            ServiceNamespace='rds',\n",
    "            ResourceId=resource_id,\n",
    "            ScalableDimension='rds:cluster:ReadReplicaCount',\n",
    "            MinCapacity=0,\n",
    "            MaxCapacity=3,\n",
    "            Tags=[\n",
    "                {\n",
    "                    'Key': 'CreationSource',\n",
    "                    'Value': 'aws-database-cookbook-v2025.8'\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Create scaling policy with low CPU threshold\n",
    "        print(\"‚öôÔ∏è Creating scaling policy with 20% CPU threshold...\")\n",
    "        policy_response = autoscaling.put_scaling_policy(\n",
    "            PolicyName=f\"{cluster_identifier}-cpu-scaling-policy\",\n",
    "            ServiceNamespace='rds',\n",
    "            ResourceId=resource_id,\n",
    "            ScalableDimension='rds:cluster:ReadReplicaCount',\n",
    "            PolicyType='TargetTrackingScaling',\n",
    "            TargetTrackingScalingPolicyConfiguration={\n",
    "                'TargetValue': 20.0,  # Low threshold for testing\n",
    "                'PredefinedMetricSpecification': {\n",
    "                    'PredefinedMetricType': 'RDSReaderAverageCPUUtilization'\n",
    "                },\n",
    "                'ScaleOutCooldown': 300,\n",
    "                'ScaleInCooldown': 300\n",
    "            },\n",
    "            Tags=[\n",
    "                {\n",
    "                    'Key': 'CreationSource',\n",
    "                    'Value': 'aws-database-cookbook-v2025.8'\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Auto scaling configured successfully!\")\n",
    "        print(\"üìà Scale-out threshold: 20% CPU utilization\")\n",
    "        print(\"üìâ Scale-in threshold: Below 20% CPU utilization\")\n",
    "        print(\"üîÑ Cooldown period: 5 minutes\")\n",
    "        \n",
    "        return policy_response['PolicyARN']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error configuring auto scaling: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Configure auto scaling\n",
    "policy_arn = configure_aurora_autoscaling(cluster_id, aws_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate CPU Intensive Workload\n",
    "\n",
    "Now we'll use pgbench to generate a CPU-intensive workload that will trigger Aurora auto scaling. We'll run multiple concurrent connections to increase CPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_pgbench_database():\n",
    "    \"\"\"Initialize pgbench database for load testing\"\"\"\n",
    "    print(\"üîß Setting up pgbench database...\")\n",
    "    \n",
    "    # Initialize pgbench tables\n",
    "    init_cmd = [\n",
    "        'pgbench',\n",
    "        '-i',  # Initialize\n",
    "        '-s', '10',  # Scale factor\n",
    "        '-h', db_host,\n",
    "        '-U', db_username,\n",
    "        '-d', db_name\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        env = os.environ.copy()\n",
    "        env['PGPASSWORD'] = db_password\n",
    "        \n",
    "        result = subprocess.run(init_cmd, env=env, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ pgbench database initialized successfully!\")\n",
    "            print(\"üìä Created tables: pgbench_accounts, pgbench_branches, pgbench_history, pgbench_tellers\")\n",
    "        else:\n",
    "            print(f\"‚ùå pgbench initialization failed: {result.stderr}\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå pgbench not found. Install PostgreSQL client tools first.\")\n",
    "        print(\"üí° Run: sudo apt-get install postgresql-client (Ubuntu) or brew install postgresql (macOS)\")\n",
    "\n",
    "def run_cpu_intensive_workload(duration_minutes=10):\n",
    "    \"\"\"Run CPU-intensive pgbench workload\"\"\"\n",
    "    print(f\"üöÄ Starting CPU-intensive workload for {duration_minutes} minutes...\")\n",
    "    print(\"üìà This will generate high CPU utilization to trigger auto scaling\")\n",
    "    \n",
    "    # High-intensity pgbench command\n",
    "    bench_cmd = [\n",
    "        'pgbench',\n",
    "        '-c', '50',  # 50 concurrent clients\n",
    "        '-j', '10',  # 10 threads\n",
    "        '-T', str(duration_minutes * 60),  # Duration in seconds\n",
    "        '-S',  # Select-only transactions (read-heavy)\n",
    "        '-h', db_host,\n",
    "        '-U', db_username,\n",
    "        '-d', db_name\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        env = os.environ.copy()\n",
    "        env['PGPASSWORD'] = db_password\n",
    "        \n",
    "        print(\"‚ö° Running high-concurrency read workload...\")\n",
    "        print(\"üìä 50 concurrent clients, 10 threads, SELECT-only queries\")\n",
    "        \n",
    "        # Run pgbench in background\n",
    "        process = subprocess.Popen(bench_cmd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        \n",
    "        return process\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running workload: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Setup and start workload\n",
    "setup_pgbench_database()\n",
    "workload_process = run_cpu_intensive_workload(10)  # 10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Monitor Auto Scaling Events\n",
    "\n",
    "While the workload is running, we'll monitor Aurora auto scaling events and CPU metrics to observe the scaling behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_scaling_events(cluster_identifier, region='us-east-1', duration_minutes=10):\n",
    "    \"\"\"Monitor Aurora auto scaling events and metrics\"\"\"\n",
    "    rds = boto3.client('rds', region_name=region)\n",
    "    cloudwatch = boto3.client('cloudwatch', region_name=region)\n",
    "    autoscaling = boto3.client('application-autoscaling', region_name=region)\n",
    "    \n",
    "    print(\"üìä Monitoring Aurora auto scaling events...\")\n",
    "    print(f\"‚è±Ô∏è Monitoring for {duration_minutes} minutes\")\n",
    "    \n",
    "    start_time = datetime.utcnow()\n",
    "    end_time = start_time + timedelta(minutes=duration_minutes)\n",
    "    \n",
    "    while datetime.utcnow() < end_time:\n",
    "        try:\n",
    "            # Get current cluster status\n",
    "            cluster_response = rds.describe_db_clusters(DBClusterIdentifier=cluster_identifier)\n",
    "            cluster = cluster_response['DBClusters'][0]\n",
    "            instance_count = len(cluster['DBClusterMembers'])\n",
    "            \n",
    "            # Get CPU metrics\n",
    "            cpu_response = cloudwatch.get_metric_statistics(\n",
    "                Namespace='AWS/RDS',\n",
    "                MetricName='CPUUtilization',\n",
    "                Dimensions=[\n",
    "                    {'Name': 'DBClusterIdentifier', 'Value': cluster_identifier}\n",
    "                ],\n",
    "                StartTime=datetime.utcnow() - timedelta(minutes=5),\n",
    "                EndTime=datetime.utcnow(),\n",
    "                Period=300,\n",
    "                Statistics=['Average']\n",
    "            )\n",
    "            \n",
    "            avg_cpu = 0\n",
    "            if cpu_response['Datapoints']:\n",
    "                avg_cpu = cpu_response['Datapoints'][-1]['Average']\n",
    "            \n",
    "            # Get scaling activities\n",
    "            scaling_response = autoscaling.describe_scaling_activities(\n",
    "                ServiceNamespace='rds',\n",
    "                ResourceId=f\"cluster:{cluster_identifier}\",\n",
    "                ScalableDimension='rds:cluster:ReadReplicaCount',\n",
    "                MaxResults=5\n",
    "            )\n",
    "            \n",
    "            current_time = datetime.utcnow().strftime('%H:%M:%S')\n",
    "            print(f\"\\r[{current_time}] Instances: {instance_count} | CPU: {avg_cpu:.1f}% | Scaling Events: {len(scaling_response['ScalingActivities'])}\", end='', flush=True)\n",
    "            \n",
    "            # Show recent scaling events\n",
    "            for activity in scaling_response['ScalingActivities'][:2]:\n",
    "                if activity['CreationTime'] > start_time:\n",
    "                    print(f\"üîÑ Scaling Event: {activity['Description']} - {activity['StatusCode']}\")\n",
    "            \n",
    "            time.sleep(30)  # Check every 30 seconds\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error monitoring: {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    print(\"‚úÖ Monitoring completed!\")\n",
    "\n",
    "def get_final_scaling_summary(cluster_identifier, region='us-east-1'):\n",
    "    \"\"\"Display final scaling summary\"\"\"\n",
    "    rds = boto3.client('rds', region_name=region)\n",
    "    autoscaling = boto3.client('application-autoscaling', region_name=region)\n",
    "    \n",
    "    try:\n",
    "        # Final cluster status\n",
    "        cluster_response = rds.describe_db_clusters(DBClusterIdentifier=cluster_identifier)\n",
    "        cluster = cluster_response['DBClusters'][0]\n",
    "        \n",
    "        # Recent scaling activities\n",
    "        scaling_response = autoscaling.describe_scaling_activities(\n",
    "            ServiceNamespace='rds',\n",
    "            ResourceId=f\"cluster:{cluster_identifier}\",\n",
    "            ScalableDimension='rds:cluster:ReadReplicaCount',\n",
    "            MaxResults=10\n",
    "        )\n",
    "        \n",
    "        print(\"üìä Final Aurora Auto Scaling Summary:\")\n",
    "        print(f\"Total Instances: {len(cluster['DBClusterMembers'])}\")\n",
    "        print(f\"Scaling Activities: {len(scaling_response['ScalingActivities'])}\")\n",
    "        \n",
    "        if scaling_response['ScalingActivities']:\n",
    "            print(\"üîÑ Recent Scaling Events:\")\n",
    "            for activity in scaling_response['ScalingActivities'][:5]:\n",
    "                event_time = activity['CreationTime'].strftime('%H:%M:%S')\n",
    "                print(f\"  [{event_time}] {activity['Description']} - {activity['StatusCode']}\")\n",
    "        \n",
    "        return cluster\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error getting summary: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Start monitoring (run this while workload is active)\n",
    "print(\"üöÄ Starting monitoring - this will run for 10 minutes...\")\n",
    "monitor_scaling_events(cluster_id, aws_region, 10)\n",
    "\n",
    "# Get final summary\n",
    "get_final_scaling_summary(cluster_id, aws_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Read Replicas\n",
    "\n",
    "The following functions demonstrate manual read replica creation and cluster endpoint management. With auto scaling enabled, Aurora will automatically create and remove read replicas based on CPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_read_replica(cluster_identifier, replica_identifier, instance_class, region='us-east-1'):\n",
    "    \"\"\"Create Aurora read replica for horizontal scaling\"\"\"\n",
    "    rds = boto3.client('rds', region_name=region)\n",
    "    \n",
    "    try:\n",
    "        print(f\"Creating read replica: {replica_identifier}\")\n",
    "        print(f\"Instance class: {instance_class}\")\n",
    "        \n",
    "        response = rds.create_db_instance(\n",
    "            DBInstanceIdentifier=replica_identifier,\n",
    "            DBInstanceClass=instance_class,\n",
    "            Engine='aurora-postgresql',\n",
    "            DBClusterIdentifier=cluster_identifier,\n",
    "            PubliclyAccessible=False,\n",
    "            Tags=[\n",
    "                {\n",
    "                    'Key': 'CreationSource',\n",
    "                    'Value': 'aws-database-cookbook-v2025.8'\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Read replica creation initiated!\")\n",
    "        print(\"‚è≥ This will take 5-10 minutes to complete...\")\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating read replica: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_cluster_endpoints(cluster_identifier, region='us-east-1'):\n",
    "    \"\"\"Display cluster endpoints for read/write routing\"\"\"\n",
    "    rds = boto3.client('rds', region_name=region)\n",
    "    \n",
    "    try:\n",
    "        response = rds.describe_db_clusters(DBClusterIdentifier=cluster_identifier)\n",
    "        cluster = response['DBClusters'][0]\n",
    "        \n",
    "        endpoints = {\n",
    "            'Endpoint Type': ['Writer Endpoint', 'Reader Endpoint'],\n",
    "            'URL': [\n",
    "                cluster.get('Endpoint', 'N/A'),\n",
    "                cluster.get('ReaderEndpoint', 'N/A')\n",
    "            ],\n",
    "            'Purpose': [\n",
    "                'Write operations (INSERT, UPDATE, DELETE)',\n",
    "                'Read operations (SELECT) - Load balanced'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(endpoints)\n",
    "        display(HTML(df.to_html(index=False)))\n",
    "        \n",
    "        print(f\"üìä Cluster has {len(cluster['DBClusterMembers'])} instances\")\n",
    "        print(\"üîÑ Reader endpoint automatically distributes read traffic\")\n",
    "        \n",
    "        return cluster\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error getting cluster info: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Show current cluster configuration\n",
    "print(\"Current Cluster Configuration:\")\n",
    "get_cluster_endpoints(cluster_id, aws_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection Routing and Load Balancing\n",
    "\n",
    "After auto scaling creates read replicas, Aurora automatically load balances read traffic across all available read replicas using the reader endpoint. The writer endpoint always routes to the primary instance for write operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Resources\n",
    "\n",
    "After testing, clean up the auto scaling configuration and any additional read replicas to avoid unnecessary costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_autoscaling(cluster_identifier, region='us-east-1'):\n",
    "    \"\"\"Remove auto scaling configuration\"\"\"\n",
    "    autoscaling = boto3.client('application-autoscaling', region_name=region)\n",
    "    \n",
    "    try:\n",
    "        resource_id = f\"cluster:{cluster_identifier}\"\n",
    "        \n",
    "        # Delete scaling policies\n",
    "        print(\"üßΩ Removing scaling policies...\")\n",
    "        autoscaling.delete_scaling_policy(\n",
    "            PolicyName=f\"{cluster_identifier}-cpu-scaling-policy\",\n",
    "            ServiceNamespace='rds',\n",
    "            ResourceId=resource_id,\n",
    "            ScalableDimension='rds:cluster:ReadReplicaCount'\n",
    "        )\n",
    "        \n",
    "        # Deregister scalable target\n",
    "        print(\"üßΩ Deregistering scalable target...\")\n",
    "        autoscaling.deregister_scalable_target(\n",
    "            ServiceNamespace='rds',\n",
    "            ResourceId=resource_id,\n",
    "            ScalableDimension='rds:cluster:ReadReplicaCount'\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Auto scaling configuration removed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during cleanup: {str(e)}\")\n",
    "\n",
    "# Uncomment to cleanup (run after testing)\n",
    "# cleanup_autoscaling(cluster_id, aws_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "1. **Update Configuration**: Modify the configuration variables at the top with your Aurora cluster details\n",
    "2. **Run Step 1**: Configure auto scaling with low CPU threshold (20%)\n",
    "3. **Run Step 2**: Generate CPU-intensive workload using pgbench\n",
    "4. **Run Step 3**: Monitor auto scaling events in real-time\n",
    "5. **Observe Results**: Watch as Aurora automatically creates read replicas when CPU exceeds 20%\n",
    "6. **Cleanup**: Remove auto scaling configuration after testing\n",
    "\n",
    "**Expected Behavior:**\n",
    "- CPU utilization will increase due to pgbench workload\n",
    "- When CPU exceeds 20%, Aurora will create additional read replicas\n",
    "- Read traffic will be automatically distributed across replicas\n",
    "- When workload decreases, Aurora will scale in (remove replicas)\n",
    "\n",
    "**Prerequisites:**\n",
    "- PostgreSQL client tools (pgbench) installed\n",
    "- Aurora PostgreSQL cluster running\n",
    "- Appropriate IAM permissions for RDS and Application Auto Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources üìö\n",
    "\n",
    "### Aurora Read Replicas & Auto Scaling\n",
    "- [Aurora Read Replicas](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html)\n",
    "- [Aurora Auto Scaling](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html)\n",
    "- [Read Replica Best Practices](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.BestPractices.html)\n",
    "\n",
    "### Performance Testing\n",
    "- [pgbench Documentation](https://www.postgresql.org/docs/current/pgbench.html)\n",
    "- [Aurora Performance Testing](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Performance.html)\n",
    "- [Load Testing Best Practices](https://docs.aws.amazon.com/wellarchitected/latest/performance-efficiency-pillar/test-performance.html)\n",
    "\n",
    "### Monitoring & Scaling\n",
    "- [CloudWatch Metrics for Aurora](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/MonitoringAurora.html)\n",
    "- [Application Auto Scaling](https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html)\n",
    "- [Performance Insights](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_PerfInsights.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
