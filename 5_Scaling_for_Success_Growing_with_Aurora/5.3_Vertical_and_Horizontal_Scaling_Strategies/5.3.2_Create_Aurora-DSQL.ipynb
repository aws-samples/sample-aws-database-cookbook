{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-section",
   "metadata": {},
   "source": [
    "# 5.3.3 Creating Your First Serverless Distributed SQL Database - Amazon DSQL Cluster\n",
    "\n",
    "<div style=\"background-color: #f8f9fa; border: 1px solid #e9ecef; border-radius: 8px; padding: 10px; margin: 10px;\">\n",
    "<strong>üìã Workshop Contents</strong>\n",
    "<ul style=\"line-height: 1.2;\">\n",
    "<li><a href=\"#What-Well-Build\">What We'll Build</a></li>\n",
    "<li><a href=\"#Cost-Overview\">Cost Overview</a></li>\n",
    "<li><a href=\"#Best-Practices\">Best Practices</a></li>\n",
    "<li><a href=\"#Prerequisites\">Prerequisites</a></li>\n",
    "<li><a href=\"#Advanced-Application-Multi-Region-Rewards-System\">Advanced Application: Multi-Region Rewards System</a></li>\n",
    "<li><a href=\"#Visual-Guide-to-Create-Multi-Region-Amazon-DSQL-Cluster\">Visual Guide to Create Multi-Region Amazon DSQL Cluster</a></li>\n",
    "<li><a href=\"#Step-1:-Check-IAM-Permissions\">Step 1: Check IAM Permissions</a></li>\n",
    "<li><a href=\"#Step-2:-Create-Multi-Region-Amazon-DSQL-Cluster\">Step 2: Create Multi-Region Amazon DSQL Cluster</a></li>\n",
    "<li><a href=\"#Step-3:-Check-Pre-installed-Dependencies\">Step 3: Check Pre-installed Dependencies</a></li>\n",
    "<li><a href=\"#Step-4:-Connect-to-Amazon-Aurora-DSQL\">Step 4: Connect to Amazon Aurora DSQL</a></li>\n",
    "<li><a href=\"#Step-5:-Create-Schema-and-Load-Sample-Data\">Step 5: Create Schema and Load Sample Data</a></li>\n",
    "<li><a href=\"#Step-6:-Test-Queries-with-Sample-Data\">Step 6: Test Queries with Sample Data</a></li>\n",
    "<li><a href=\"#Summary-and-Next-Steps\">Summary and Next Steps</a></li>\n",
    "<li><a href=\"#Additional-Resources-üìö\">Additional Resources</a></li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Let's create a multi-region Amazon DSQL cluster step by step! üöÄ\n",
    "\n",
    "> **üí° What is Amazon Aurora DSQL?**: [Amazon Aurora DSQL](https://aws.amazon.com/rds/aurora/dsql/) is a serverless, distributed SQL database that provides strong consistency across multiple regions with PostgreSQL compatibility. It automatically scales to handle your workload and provides built-in high availability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what-we-build",
   "metadata": {},
   "source": [
    "## What We'll Build\n",
    "\n",
    "- **Multi-Region Setup**: Amazon DSQL clusters in us-east-1 and us-east-2\n",
    "- **Active-Active Configuration**: Strong consistency across all regions\n",
    "- **Sample Application**: Retail rewards points system with PostgreSQL compatibility\n",
    "- **IAM Authentication**: Secure connections without managing passwords\n",
    "\n",
    "## Cost Overview üí∞\n",
    "\n",
    "| Component | Cost | Notes |\n",
    "|-----------|------|-------|\n",
    "| Amazon DSQL | Pay-per-use | No minimum charges, scales to zero |\n",
    "| Data Transfer | Standard rates | Cross-region replication included |\n",
    "| Storage | Pay for what you use | Automatic scaling and optimization |\n",
    "\n",
    "üí° **Cost Optimization Tips:**\n",
    "- Amazon DSQL automatically scales to zero when not in use\n",
    "- No capacity planning required - pay only for actual usage\n",
    "- Built-in multi-region replication without additional charges\n",
    "- Serverless architecture eliminates idle resource costs\n",
    "\n",
    "## Best Practices ‚úÖ\n",
    "\n",
    "1. **Multi-Region Design**\n",
    "   - Deploy in multiple regions for high availability\n",
    "   - Use local endpoints for optimal performance\n",
    "   - Leverage strong consistency guarantees\n",
    "\n",
    "2. **Security**\n",
    "   - Use IAM authentication for secure access\n",
    "   - Enable encryption in transit and at rest\n",
    "   - Follow principle of least privilege\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisites",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before starting this tutorial, ensure you have:\n",
    "\n",
    "- **AWS Account**: Active AWS account with appropriate permissions\n",
    "- **AWS CLI**: Configured with credentials and default region\n",
    "- **IAM Permissions**: Required Amazon DSQL permissions (we'll verify these in Step 1)\n",
    "- **Jupyter Notebook**: You can launch a [free tier Amazon SageMaker Jupyter Notebook](../../1_Getting_Started_with_AWS/1.4_Setting_up_Your_Cookbook_Environment/README.MD)\n",
    "- **Python Environment**: Python 3.6+ with pip for installing packages\n",
    "- **Basic Knowledge**: Familiarity with SQL and AWS services\n",
    "\n",
    "### Required IAM Permissions\n",
    "\n",
    "Your IAM user or role needs the following permissions:\n",
    "- `dsql:*` - Full Amazon DSQL permissions\n",
    "- `[RDS:GenerateDbAuthToken` - For IAM database authentication\n",
    "- `sts:GetCallerIdentity` - To verify your AWS identity\n",
    "\n",
    "> **üí° Tip**: If you completed earlier modules in this cookbook, you likely already have the necessary permissions configured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-application",
   "metadata": {},
   "source": [
    "## Advanced Application: Multi-Region Rewards System\n",
    "\n",
    "<div style=\"padding: 15px; background-color: #e6f7e6; border-left: 5px solid #28a745; margin-bottom: 10px;\">\n",
    "<strong>üöÄ Advanced Use Case:</strong> You can enhance the serverless web application from <a href=\"../../3_Building_Your_First_Serverless_Web_App_with_Aurora/README.MD\">Module 3: Building Your First Serverless Web App with Aurora</a> by replacing the Aurora database with Amazon DSQL. This upgrade transforms your application into a multi-region resilient system that can withstand regional failures while maintaining strong data consistency across regions.\n",
    "</div>\n",
    "\n",
    "### Application Architecture\n",
    "\n",
    "The retail rewards points system demonstrates Amazon DSQL's capabilities:\n",
    "\n",
    "- **Frontend**: AWS Amplify hosting the web interface\n",
    "- **API Layer**: API Gateway with AWS Lambda functions\n",
    "- **Authentication**: Amazon Cognito user pools\n",
    "- **Database**: Amazon DSQL with multi-region active-active setup\n",
    "- **Authorization**: IAM for fine-grained access control\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "- **Regional Resilience**: Application continues operating even if one region fails\n",
    "- **Strong Consistency**: Data remains consistent across all regions\n",
    "- **Transparent Scaling**: Amazon DSQL handles scaling automatically\n",
    "- **Cost Optimization**: Pay only for actual usage with serverless architecture\n",
    "\n",
    "![Retail rewards points system powered by Aurora DSQL](../images/5.3-retail-awards-architecture-aurora-dsql.png)\n",
    "\n",
    "Now, let's create an Amazon DSQL cluster to power this architecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a55d00",
   "metadata": {},
   "source": [
    "## Visual Guide to Amazon Aurora DSQL Console\n",
    "\n",
    "![Creating Amazon Aurora DSQL Cluster](../images/5.3-create-aurora-dsql.gif)\n",
    "*Step-by-step console walkthrough of creating an Amazon DSQL cluster*\n",
    "\n",
    "> **üí° Note**: You can also follow the step-by-step guidance as follows to create Amazon Aurora DSQL through CLIs. Otherwise, jump to <a href=\"#Step-4:-Connect-to-Amazon-Aurora-DSQL\">Step 4: Connect to Amazon Aurora DSQL</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-check-permissions",
   "metadata": {},
   "source": [
    "## Step 1: Check IAM Permissions\n",
    "\n",
    "First, we'll upgrade the AWS CLI to support DSQL commands, then verify your IAM permissions for Amazon DSQL operations. The SageMaker environment has an older AWS CLI version that doesn't include DSQL support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-permissions",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"üîç Checking IAM permissions for Aurora DSQL...\"\n",
    "\n",
    "# Check if we can list DSQL clusters (this will test basic permissions)\n",
    "if aws dsql list-clusters --region us-east-1 > /dev/null 2>&1; then\n",
    "    echo \"‚úÖ Aurora DSQL permissions verified\"\n",
    "else\n",
    "    echo \"‚ùå Missing Aurora DSQL permissions\"\n",
    "    echo \"Required IAM policy:\"\n",
    "    cat << 'EOF'\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"dsql:CreateCluster\",\n",
    "        \"dsql:GetCluster\",\n",
    "        \"dsql:UpdateCluster\",\n",
    "        \"dsql:DeleteCluster\",\n",
    "        \"dsql:ListClusters\",\n",
    "        \"dsql:CreateMultiRegionClusters\",\n",
    "        \"dsql:DeleteMultiRegionClusters\",\n",
    "        \"dsql:DbConnect\",\n",
    "        \"dsql:DbConnectAdmin\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "EOF\n",
    "fi\n",
    "\n",
    "# Get current identity\n",
    "echo \"Current AWS identity:\"\n",
    "aws sts get-caller-identity --output table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-create-cluster",
   "metadata": {},
   "source": [
    "## Step 2: Create Multi-Region Amazon DSQL Cluster\n",
    "\n",
    "This step creates a true multi-region DSQL cluster with active-active replication between us-east-1 and us-east-2, using us-west-2 as the witness region. The process involves creating two clusters and configuring them as peers for strong consistency across regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-primary-cluster",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"üöÄ Creating multi-region DSQL clusters with proper peering...\"\n",
    "\n",
    "# Create first cluster in us-east-1 with witness region\n",
    "CLUSTER_1_ARN=$(aws dsql create-cluster \\\n",
    "    --region us-east-1 \\\n",
    "    --tags Name=\"Rewards Multi-Region\" CreationSource=\"aws-database-cookbook-v2025.8\" \\\n",
    "    --multi-region-properties witnessRegion=us-west-2 \\\n",
    "    --query 'arn' --output text)\n",
    "\n",
    "echo \"‚úÖ Created cluster 1: $CLUSTER_1_ARN\"\n",
    "CLUSTER_1_ID=$(echo $CLUSTER_1_ARN | cut -d'/' -f2)\n",
    "\n",
    "# Create second cluster in us-east-2 with witness region and peer\n",
    "CLUSTER_2_ARN=$(aws dsql create-cluster \\\n",
    "    --region us-east-2 \\\n",
    "    --tags Name=\"Rewards Multi-Region\" CreationSource=\"aws-database-cookbook-v2025.8\" \\\n",
    "    --multi-region-properties witnessRegion=us-west-2,clusters=[$CLUSTER_1_ARN] \\\n",
    "    --query 'arn' --output text)\n",
    "\n",
    "echo \"‚úÖ Created cluster 2: $CLUSTER_2_ARN\"\n",
    "CLUSTER_2_ID=$(echo $CLUSTER_2_ARN | cut -d'/' -f2)\n",
    "\n",
    "# Update first cluster to peer with second cluster\n",
    "aws dsql update-cluster \\\n",
    "    --identifier $CLUSTER_1_ID \\\n",
    "    --region us-east-1 \\\n",
    "    --multi-region-properties witnessRegion=us-west-2,clusters=[$CLUSTER_2_ARN]\n",
    "\n",
    "echo \"üîó Peered clusters for multi-region setup\"\n",
    "\n",
    "# Wait for clusters to become active\n",
    "echo \"‚è≥ Waiting for clusters to become ACTIVE...\"\n",
    "\n",
    "while true; do\n",
    "    STATUS_1=$(aws dsql get-cluster --identifier $CLUSTER_1_ID --region us-east-1 --query 'status' --output text)\n",
    "    STATUS_2=$(aws dsql get-cluster --identifier $CLUSTER_2_ID --region us-east-2 --query 'status' --output text)\n",
    "    echo \"Cluster 1 status: $STATUS_1, Cluster 2 status: $STATUS_2\"\n",
    "    if [ \"$STATUS_1\" = \"ACTIVE\" ] && [ \"$STATUS_2\" = \"ACTIVE\" ]; then\n",
    "        break\n",
    "    fi\n",
    "    sleep 60\n",
    "done\n",
    "\n",
    "# Save cluster information\n",
    "echo \"export PRIMARY_CLUSTER_ARN=$CLUSTER_1_ARN\" > .dsql_cluster_info\n",
    "echo \"export PRIMARY_CLUSTER_ID=$CLUSTER_1_ID\" >> .dsql_cluster_info\n",
    "echo \"export SECONDARY_CLUSTER_ARN=$CLUSTER_2_ARN\" >> .dsql_cluster_info\n",
    "echo \"export SECONDARY_CLUSTER_ID=$CLUSTER_2_ID\" >> .dsql_cluster_info\n",
    "echo \"export PRIMARY_ENDPOINT=$CLUSTER_1_ID.dsql.us-east-1.on.aws\" >> .dsql_cluster_info\n",
    "echo \"export SECONDARY_ENDPOINT=$CLUSTER_2_ID.dsql.us-east-2.on.aws\" >> .dsql_cluster_info\n",
    "\n",
    "echo \"‚úÖ Multi-region DSQL clusters are now ACTIVE!\"\n",
    "echo \"Primary: $CLUSTER_1_ID.dsql.us-east-1.on.aws\"\n",
    "echo \"Secondary: $CLUSTER_2_ID.dsql.us-east-2.on.aws\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2b-create-secondary-cluster",
   "metadata": {},
   "source": [
    "### Multi-Region Configuration Summary\n",
    "\n",
    "The multi-region DSQL cluster provides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-secondary-cluster",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Display actual cluster configuration\n",
    "source .dsql_cluster_info\n",
    "\n",
    "echo \"üìã Multi-Region DSQL Configuration:\"\n",
    "echo \"- Primary Region: us-east-1 (N.Virginia)\"\n",
    "echo \"- Secondary Region: us-east-2 (Ohio)\"\n",
    "echo \"- Witness Region: us-west-2 (Oregon)\"\n",
    "echo \"- Architecture: Active-Active Multi-Region\"\n",
    "echo \"- Consistency: Strong consistency across all regions\"\n",
    "\n",
    "echo \"üîç Cluster Details:\"\n",
    "aws dsql get-cluster --identifier $PRIMARY_CLUSTER_ID --region us-east-1 --query '{Status:status,Arn:arn}' --output table\n",
    "aws dsql get-cluster --identifier $SECONDARY_CLUSTER_ID --region us-east-2 --query '{Status:status,Arn:arn}' --output table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-install-dependencies",
   "metadata": {},
   "source": [
    "## Step 3: Check Pre-installed Dependencies\n",
    "\n",
    "The SageMaker notebook environment deployed by the CloudFormation template includes pre-installed packages. Let's verify the availability of required dependencies instead of installing them directly. The SageMaker lifecycle configuration automatically installs psycopg[binary], boto3, and pandas during notebook startup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-dependencies",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def check_package(package_name):\n",
    "    \"\"\"Check if a package is installed\"\"\"\n",
    "    try:\n",
    "        __import__(package_name)\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "print(\"üì¶ Checking pre-installed dependencies from SageMaker environment...\")\n",
    "\n",
    "# Check required packages\n",
    "packages = {\n",
    "    'boto3': 'AWS SDK for Python',\n",
    "    'psycopg': 'PostgreSQL adapter for Python',\n",
    "    'pandas': 'Data manipulation and analysis library'\n",
    "}\n",
    "\n",
    "all_available = True\n",
    "for package, description in packages.items():\n",
    "    if check_package(package):\n",
    "        print(f\"‚úÖ {package}: {description} - Available\")\n",
    "    else:\n",
    "        print(f\"‚ùå {package}: {description} - Not available\")\n",
    "        all_available = False\n",
    "\n",
    "if all_available:\n",
    "    print(\"‚úÖ All required dependencies are pre-installed in the SageMaker environment!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Some packages are missing. Installing now...\")\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'psycopg[binary]', 'boto3', 'pandas'])\n",
    "    print(\"‚úÖ Missing dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-connect-dsql",
   "metadata": {},
   "source": [
    "## Step 4: Connect to Amazon Aurora DSQL\n",
    "\n",
    "The following script defines essential Python functions for Amazon DSQL connectivity, including loading cluster information from the saved file, generating IAM authentication tokens, and establishing secure database connections. The functions handle error cases gracefully and provide clear status messages during connection attempts. These utilities will be used throughout the remaining steps for all database operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-connection-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import psycopg\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load cluster information\n",
    "def load_cluster_info():\n",
    "    \"\"\"Load cluster information from environment file\"\"\"\n",
    "    cluster_info = {}\n",
    "    try:\n",
    "        with open('.dsql_cluster_info', 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('export '):\n",
    "                    key, value = line.replace('export ', '').strip().split('=', 1)\n",
    "                    cluster_info[key] = value\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Cluster info file not found. Please run the cluster creation steps first.\")\n",
    "        return None\n",
    "    return cluster_info\n",
    "\n",
    "# Get IAM authentication token\n",
    "def get_iam_token(endpoint, region):\n",
    "    \"\"\"Generate IAM authentication token for Aurora DSQL\"\"\"\n",
    "    try:\n",
    "        # Use DSQL client for admin token generation\n",
    "        dsql_client = boto3.client('dsql', region_name=region)\n",
    "        token = dsql_client.generate_db_connect_admin_auth_token(endpoint, region)\n",
    "        return token\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating IAM token: {e}\")\n",
    "        return None\n",
    "\n",
    "# Connect to Aurora DSQL\n",
    "def connect_to_dsql(endpoint, region, database='postgres'):\n",
    "    \"\"\"Connect to Aurora DSQL using IAM authentication\"\"\"\n",
    "    print(f\"üîå Connecting to Aurora DSQL at {endpoint}...\")\n",
    "    \n",
    "    # Use admin user for DSQL connection\n",
    "    try:\n",
    "        username = 'admin'\n",
    "        print(f\"Using DSQL admin user: {username}\")\n",
    "        \n",
    "        # Get IAM token\n",
    "        token = get_iam_token(endpoint, region)\n",
    "        if not token:\n",
    "            return None\n",
    "        \n",
    "        # Try connection with IAM authentication\n",
    "        conn = psycopg.connect(\n",
    "            host=endpoint,\n",
    "            port=5432,\n",
    "            dbname=database,\n",
    "            user=username,\n",
    "            password=token,\n",
    "            sslmode='require',\n",
    "            connect_timeout=30\n",
    "        )\n",
    "        print(f\"‚úÖ Successfully connected to {region}\")\n",
    "        return conn\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Connection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"üîß Connection functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8-create-demo-data",
   "metadata": {},
   "source": [
    "## Step 5: Create Schema and Load Sample Data\n",
    "\n",
    "This step creates a retail rewards database schema and loads sample data into your Amazon DSQL cluster. We'll create tables for customers, products, orders, and rewards to demonstrate cross-region replication capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-demo-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for SQL files if it doesn't exist\n",
    "import os\n",
    "\n",
    "# Create sql directory if it doesn't exist\n",
    "os.makedirs('./sql', exist_ok=True)\n",
    "\n",
    "# Create schema file if it doesn't exist\n",
    "schema_file = './sql/aurora_dsql_schema.sql'\n",
    "if not os.path.exists(schema_file):\n",
    "    print(f\"Creating schema file: {schema_file}\")\n",
    "    with open(schema_file, 'w') as f:\n",
    "        f.write('''\n",
    "-- Create schema for retail rewards application\n",
    "CREATE SCHEMA IF NOT EXISTS xpoints;\n",
    "\n",
    "-- Create customers table\n",
    "CREATE TABLE IF NOT EXISTS xpoints.customers (\n",
    "    customer_id VARCHAR(36) PRIMARY KEY,\n",
    "    first_name VARCHAR(50) NOT NULL,\n",
    "    last_name VARCHAR(50) NOT NULL,\n",
    "    email VARCHAR(100) UNIQUE NOT NULL,\n",
    "    phone VARCHAR(20),\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    region VARCHAR(20) NOT NULL\n",
    ");\n",
    "\n",
    "-- Create products table\n",
    "CREATE TABLE IF NOT EXISTS xpoints.products (\n",
    "    product_id VARCHAR(36) PRIMARY KEY,\n",
    "    name VARCHAR(100) NOT NULL,\n",
    "    description TEXT,\n",
    "    price DECIMAL(10, 2) NOT NULL,\n",
    "    category VARCHAR(50) NOT NULL,\n",
    "    inventory_count INT NOT NULL DEFAULT 0,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    region VARCHAR(20) NOT NULL\n",
    ");\n",
    "\n",
    "-- Create orders table\n",
    "CREATE TABLE IF NOT EXISTS xpoints.orders (\n",
    "    order_id VARCHAR(36) PRIMARY KEY,\n",
    "    customer_id VARCHAR(36) NOT NULL REFERENCES xpoints.customers(customer_id),\n",
    "    order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    status VARCHAR(20) NOT NULL,\n",
    "    total_amount DECIMAL(10, 2) NOT NULL,\n",
    "    shipping_address TEXT,\n",
    "    region VARCHAR(20) NOT NULL,\n",
    "    CONSTRAINT valid_status CHECK (status IN (''pending'', ''processing'', ''shipped'', ''delivered'', ''cancelled''))\n",
    ");\n",
    "\n",
    "-- Create order_items table\n",
    "CREATE TABLE IF NOT EXISTS xpoints.order_items (\n",
    "    order_item_id VARCHAR(36) PRIMARY KEY,\n",
    "    order_id VARCHAR(36) NOT NULL REFERENCES xpoints.orders(order_id),\n",
    "    product_id VARCHAR(36) NOT NULL REFERENCES xpoints.products(product_id),\n",
    "    quantity INT NOT NULL,\n",
    "    price_per_unit DECIMAL(10, 2) NOT NULL\n",
    ");\n",
    "\n",
    "-- Create rewards table\n",
    "CREATE TABLE IF NOT EXISTS xpoints.rewards (\n",
    "    reward_id VARCHAR(36) PRIMARY KEY,\n",
    "    customer_id VARCHAR(36) NOT NULL REFERENCES xpoints.customers(customer_id),\n",
    "    points_balance INT NOT NULL DEFAULT 0,\n",
    "    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    region VARCHAR(20) NOT NULL\n",
    ");\n",
    "\n",
    "-- Create reward_transactions table\n",
    "CREATE TABLE IF NOT EXISTS xpoints.reward_transactions (\n",
    "    transaction_id VARCHAR(36) PRIMARY KEY,\n",
    "    reward_id VARCHAR(36) NOT NULL REFERENCES xpoints.rewards(reward_id),\n",
    "    points_change INT NOT NULL,\n",
    "    transaction_type VARCHAR(20) NOT NULL,\n",
    "    transaction_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    order_id VARCHAR(36) REFERENCES xpoints.orders(order_id),\n",
    "    notes TEXT,\n",
    "    region VARCHAR(20) NOT NULL,\n",
    "    CONSTRAINT valid_transaction_type CHECK (transaction_type IN (''earn'', ''redeem'', ''expire'', ''adjust''))\n",
    ");\n",
    "''')\n",
    "    print(f\"‚úÖ Schema file created: {schema_file}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Schema file already exists: {schema_file}\")\n",
    "\n",
    "print(\"‚úÖ SQL files prepared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cb43e7",
   "metadata": {},
   "source": [
    "Now let's create the schema and load initial data into the primary region (us-east-1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"üèóÔ∏è Creating schema and loading data...\"\n",
    "\n",
    "# Source cluster info\n",
    "source .dsql_cluster_info\n",
    "\n",
    "# Get DSQL admin token using AWS CLI\n",
    "TOKEN=$(aws dsql generate-db-connect-admin-auth-token --hostname $PRIMARY_ENDPOINT --region us-east-1 --output text)\n",
    "echo \"‚úÖ Successfully generated authentication token\"\n",
    "\n",
    "# Set connection variables\n",
    "export PGHOST=$PRIMARY_ENDPOINT\n",
    "export PGPORT=5432\n",
    "export PGDATABASE=postgres\n",
    "export PGUSER=admin\n",
    "export PGPASSWORD=$TOKEN\n",
    "export PGSSLMODE=require\n",
    "\n",
    "# Create schema\n",
    "echo \"Creating schema...\"\n",
    "psql -f ./sql/aurora_dsql_schema.sql\n",
    "\n",
    "echo \"‚úÖ Schema creation complete!\"\n",
    "\n",
    "# Insert sample data into us-east-1 (primary region)\n",
    "echo \"üåü Inserting sample data in us-east-1 region...\"\n",
    "psql << EOF\n",
    "-- Insert customers\n",
    "INSERT INTO xpoints.customers (customer_id, first_name, last_name, email, phone, region)\n",
    "VALUES \n",
    "('c1', 'John', 'Smith', 'john.smith@example.com', '555-123-4567', 'us-east-1'),\n",
    "('c2', 'Jane', 'Doe', 'jane.doe@example.com', '555-234-5678', 'us-east-1'),\n",
    "('c3', 'Robert', 'Johnson', 'robert.j@example.com', '555-345-6789', 'us-east-1');\n",
    "\n",
    "-- Insert products\n",
    "INSERT INTO xpoints.products (product_id, name, description, price, category, inventory_count, region)\n",
    "VALUES \n",
    "('p1', 'Wireless Earbuds', 'High-quality wireless earbuds with noise cancellation', 129.99, 'Electronics', 50, 'us-east-1'),\n",
    "('p2', 'Smart Watch', 'Fitness tracking smartwatch with heart rate monitor', 199.99, 'Electronics', 30, 'us-east-1'),\n",
    "('p3', 'Portable Charger', '10000mAh portable power bank', 49.99, 'Accessories', 100, 'us-east-1');\n",
    "\n",
    "-- Insert orders\n",
    "INSERT INTO xpoints.orders (order_id, customer_id, status, total_amount, shipping_address, region)\n",
    "VALUES \n",
    "('o1', 'c1', 'delivered', 129.99, '123 Main St, Anytown, USA', 'us-east-1'),\n",
    "('o2', 'c2', 'shipped', 249.98, '456 Oak Ave, Somewhere, USA', 'us-east-1');\n",
    "\n",
    "-- Insert order items\n",
    "INSERT INTO xpoints.order_items (order_item_id, order_id, product_id, quantity, price_per_unit)\n",
    "VALUES \n",
    "('oi1', 'o1', 'p1', 1, 129.99),\n",
    "('oi2', 'o2', 'p1', 1, 129.99),\n",
    "('oi3', 'o2', 'p3', 1, 49.99);\n",
    "\n",
    "-- Insert rewards\n",
    "INSERT INTO xpoints.rewards (reward_id, customer_id, points_balance, region)\n",
    "VALUES \n",
    "('r1', 'c1', 130, 'us-east-1'),\n",
    "('r2', 'c2', 250, 'us-east-1'),\n",
    "('r3', 'c3', 0, 'us-east-1');\n",
    "\n",
    "-- Insert reward transactions\n",
    "INSERT INTO xpoints.reward_transactions (transaction_id, reward_id, points_change, transaction_type, order_id, notes, region)\n",
    "VALUES \n",
    "('t1', 'r1', 130, 'earn', 'o1', 'Points for order #o1', 'us-east-1'),\n",
    "('t2', 'r2', 250, 'earn', 'o2', 'Points for order #o2', 'us-east-1');\n",
    "\n",
    "EOF\n",
    "\n",
    "echo \"‚úÖ Sample data inserted in us-east-1 region!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step9-test-queries",
   "metadata": {},
   "source": [
    "## Step 6: Test Queries with Sample Data\n",
    "\n",
    "This step demonstrates the cross-region replication capabilities of Amazon DSQL. We'll insert data in one region, query it from another region, update data in the second region, and verify that changes are immediately visible in the first region - all with strong consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-queries",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cross_region_replication():\n",
    "    \"\"\"Test cross-region replication by inserting data in one region and querying from another\"\"\"\n",
    "    # Load cluster information\n",
    "    cluster_info = load_cluster_info()\n",
    "    if not cluster_info:\n",
    "        return\n",
    "    \n",
    "    # Get endpoints for both regions\n",
    "    primary_endpoint = cluster_info.get('PRIMARY_ENDPOINT')\n",
    "    secondary_endpoint = cluster_info.get('SECONDARY_ENDPOINT')\n",
    "    \n",
    "    print(\"üîÑ Testing cross-region replication...\")\n",
    "    print(\"\\n1Ô∏è‚É£ STEP 1: Query data from secondary region (us-east-2) to verify initial replication\")\n",
    "    \n",
    "    # Connect to secondary region\n",
    "    conn_secondary = connect_to_dsql(secondary_endpoint, 'us-east-2')\n",
    "    if not conn_secondary:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Query customers from secondary region\n",
    "        cursor = conn_secondary.cursor()\n",
    "        cursor.execute(\"SELECT customer_id, first_name, last_name, email, region FROM xpoints.customers ORDER BY customer_id\")\n",
    "        print(\"\\nüìã Customers in us-east-2 (should show data inserted in us-east-1):\")\n",
    "        results = cursor.fetchall()\n",
    "        for row in results:\n",
    "            print(f\"   {row}\")\n",
    "        \n",
    "        # Query rewards from secondary region\n",
    "        cursor.execute(\"SELECT reward_id, customer_id, points_balance, region FROM xpoints.rewards ORDER BY reward_id\")\n",
    "        print(\"\\nüìã Rewards in us-east-2 (should show data inserted in us-east-1):\")\n",
    "        results = cursor.fetchall()\n",
    "        for row in results:\n",
    "            print(f\"   {row}\")\n",
    "        \n",
    "        print(\"\\n2Ô∏è‚É£ STEP 2: Insert new data in secondary region (us-east-2)\")\n",
    "        \n",
    "        # Insert new customer in secondary region\n",
    "        cursor.execute(\"\"\"\n",
    "        INSERT INTO xpoints.customers (customer_id, first_name, last_name, email, phone, region)\n",
    "        VALUES ('c4', 'Emily', 'Wilson', 'emily.w@example.com', '555-456-7890', 'us-east-2')\n",
    "        RETURNING customer_id, first_name, last_name, email, region\n",
    "        \"\"\")\n",
    "        new_customer = cursor.fetchone()\n",
    "        print(f\"\\n‚úÖ New customer inserted in us-east-2: {new_customer}\")\n",
    "        \n",
    "        # Insert new product in secondary region\n",
    "        cursor.execute(\"\"\"\n",
    "        INSERT INTO xpoints.products (product_id, name, description, price, category, inventory_count, region)\n",
    "        VALUES ('p4', 'Bluetooth Speaker', 'Waterproof portable speaker', 79.99, 'Electronics', 45, 'us-east-2')\n",
    "        RETURNING product_id, name, price, region\n",
    "        \"\"\")\n",
    "        new_product = cursor.fetchone()\n",
    "        print(f\"‚úÖ New product inserted in us-east-2: {new_product}\")\n",
    "        \n",
    "        # Insert new reward in secondary region\n",
    "        cursor.execute(\"\"\"\n",
    "        INSERT INTO xpoints.rewards (reward_id, customer_id, points_balance, region)\n",
    "        VALUES ('r4', 'c4', 100, 'us-east-2')\n",
    "        RETURNING reward_id, customer_id, points_balance, region\n",
    "        \"\"\")\n",
    "        new_reward = cursor.fetchone()\n",
    "        print(f\"‚úÖ New reward inserted in us-east-2: {new_reward}\")\n",
    "        \n",
    "        # Commit changes\n",
    "        conn_secondary.commit()\n",
    "        cursor.close()\n",
    "        conn_secondary.close()\n",
    "        \n",
    "        print(\"\\n3Ô∏è‚É£ STEP 3: Query data from primary region (us-east-1) to verify replication from us-east-2\")\n",
    "        \n",
    "        # Connect to primary region\n",
    "        conn_primary = connect_to_dsql(primary_endpoint, 'us-east-1')\n",
    "        if not conn_primary:\n",
    "            return\n",
    "        \n",
    "        cursor = conn_primary.cursor()\n",
    "        \n",
    "        # Query customers from primary region\n",
    "        cursor.execute(\"SELECT customer_id, first_name, last_name, email, region FROM xpoints.customers WHERE customer_id = 'c4'\")\n",
    "        result = cursor.fetchone()\n",
    "        print(f\"\\nüìã Customer c4 in us-east-1 (should show data inserted in us-east-2): {result}\")\n",
    "        \n",
    "        # Query products from primary region\n",
    "        cursor.execute(\"SELECT product_id, name, price, region FROM xpoints.products WHERE product_id = 'p4'\")\n",
    "        result = cursor.fetchone()\n",
    "        print(f\"üìã Product p4 in us-east-1 (should show data inserted in us-east-2): {result}\")\n",
    "        \n",
    "        # Query rewards from primary region\n",
    "        cursor.execute(\"SELECT reward_id, customer_id, points_balance, region FROM xpoints.rewards WHERE reward_id = 'r4'\")\n",
    "        result = cursor.fetchone()\n",
    "        print(f\"üìã Reward r4 in us-east-1 (should show data inserted in us-east-2): {result}\")\n",
    "        \n",
    "        print(\"\\n4Ô∏è‚É£ STEP 4: Update data in primary region (us-east-1) and verify in secondary region (us-east-2)\")\n",
    "        \n",
    "        # Update reward points in primary region\n",
    "        cursor.execute(\"\"\"\n",
    "        UPDATE xpoints.rewards \n",
    "        SET points_balance = points_balance + 50 \n",
    "        WHERE reward_id = 'r4'\n",
    "        RETURNING reward_id, customer_id, points_balance, region\n",
    "        \"\"\")\n",
    "        updated_reward = cursor.fetchone()\n",
    "        print(f\"\\n‚úÖ Updated reward in us-east-1: {updated_reward}\")\n",
    "        \n",
    "        # Commit changes\n",
    "        conn_primary.commit()\n",
    "        cursor.close()\n",
    "        conn_primary.close()\n",
    "        \n",
    "        # Connect back to secondary region to verify update\n",
    "        conn_secondary = connect_to_dsql(secondary_endpoint, 'us-east-2')\n",
    "        if not conn_secondary:\n",
    "            return\n",
    "        \n",
    "        cursor = conn_secondary.cursor()\n",
    "        cursor.execute(\"SELECT reward_id, customer_id, points_balance, region FROM xpoints.rewards WHERE reward_id = 'r4'\")\n",
    "        result = cursor.fetchone()\n",
    "        print(f\"üìã Reward r4 in us-east-2 (should show updated points from us-east-1): {result}\")\n",
    "        \n",
    "        cursor.close()\n",
    "        conn_secondary.close()\n",
    "        \n",
    "        print(\"\\n‚úÖ Cross-region replication test completed successfully!\")\n",
    "        print(\"‚úÖ Changes made in one region are immediately visible in the other region\")\n",
    "        print(\"‚úÖ This demonstrates Aurora DSQL's strong consistency across regions\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during cross-region replication test: {e}\")\n",
    "        if conn_secondary and not conn_secondary.closed:\n",
    "            conn_secondary.close()\n",
    "\n",
    "# Run the cross-region replication test\n",
    "test_cross_region_replication()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "üéâ **Congratulations!** You have successfully:\n",
    "\n",
    "‚úÖ Created multi-region Amazon DSQL clusters in us-east-1 and us-east-2  \n",
    "‚úÖ Established active-active replication between regions  \n",
    "‚úÖ Connected to both clusters using IAM authentication  \n",
    "‚úÖ Created the rewards application schema with sample data  \n",
    "‚úÖ Tested queries across both regions  \n",
    "‚úÖ Demonstrated real-time cross-region replication with strong consistency\n",
    "\n",
    "### Key Benefits Demonstrated\n",
    "\n",
    "- **Serverless**: No capacity planning required - scales automatically\n",
    "- **PostgreSQL Compatible**: Use familiar SQL syntax and tools\n",
    "- **IAM Authentication**: Secure connections without managing passwords\n",
    "- **Multi-region active-active replication**: Changes from 1 region are seamlessly synchronized to another region\n",
    "\n",
    "### Connection Information Summary\n",
    "\n",
    "Your cluster connection details have been saved to `.dsql_cluster_info`. Use these endpoints in your applications for PostgreSQL-compatible access to your distributed SQL database.\n",
    "\n",
    "### Next Steps üöÄ\n",
    "\n",
    "1. **Explore Amazon Aurora DSQL Workshop**: You will interact with the Aurora DSQL database and run some application code from [AWS CloudShell](https://docs.aws.amazon.com/cloudshell/latest/userguide/welcome.html). CloudShell is a browser-based Linux shell that you can launch directly from the AWS Management Console. You won't have to install any software on your workstation to complete this workshop.\n",
    "2. **Create schema and load data**: After [preparing your Amazon Aurora DSQL workshop environment](https://catalog.workshops.aws/aurora-dsql/en-US/01-getting-started/01-set-up-first-region), you can create schemas and load some data that later on for you to enhance your Xanadu Rewards Application built in [Section 3. Building Your First Serverless Web App with Amazon Aurora](../../3_Building_Your_First_Serverless_Web_App_with_Aurora/README.MD) with a multi-region active-active synchronization capability. With Aurora DSQL's bi-directional synchronous replication, your application can now accept reads and writes in two AWS regions.\n",
    "3. **Explore the Data Model**: Continue with the [Amazon Amazon DSQL Workshop - Explore the Data Model](https://catalog.workshops.aws/aurora-dsql/en-US/02-working-with-aurora-dsql/04-explore-the-data-model) to learn advanced querying techniques and data modeling best practices.\n",
    "4. **Build Applications**: [Integrate Amazon DSQL into your Reward System application](https://catalog.workshops.aws/aurora-dsql/en-US/03-building-the-rewards-app) using the connection patterns demonstrated above.\n",
    "5. **Explore Migration Options**: Learn how to migrate existing PostgreSQL databases to Amazon DSQL in [5.3.3 Migrating to Amazon Aurora DSQL](./5.3.3_Migrating-to-Aurora-DSQL.ipynb)\n",
    "\n",
    "üéâ **Excellent!** You've successfully set up multi-region Amazon DSQL clusters and learned the fundamentals of distributed SQL databases. Your foundation for building globally distributed applications is now in place!\n",
    "\n",
    "**Ready to continue?** Let's advance to [6. Optimizing Performance and Cost: Mastering Aurora Efficiency](../../6_Optimizing_Performance_and_Cost/README.md) and how to fine-tune your Aurora database for maximum performance and cost-effectiveness!\n",
    "\n",
    "### Cleanup Resources üßπ\n",
    "\n",
    "When you're done experimenting, remember to clean up your resources to avoid ongoing charges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cleanup script deletes both Aurora DSQL clusters to avoid ongoing charges once you're finished with the tutorial.\n",
    "# The commands are commented out for safety - uncomment them when you're ready to permanently delete the clusters.\n",
    "# You'll see deletion confirmation messages for both the secondary and primary clusters.\n",
    "\n",
    "%%bash\n",
    "# Uncomment and run this cell to clean up resources\n",
    "# echo \"üßπ Cleaning up Aurora DSQL resources...\"\n",
    "# \n",
    "# # Source cluster info\n",
    "# source .dsql_cluster_info\n",
    "# \n",
    "# # Delete both clusters\n",
    "# echo \"Deleting secondary cluster...\"\n",
    "# aws dsql delete-cluster --identifier $SECONDARY_CLUSTER_ID --region us-east-2\n",
    "# \n",
    "# echo \"Deleting primary cluster...\"\n",
    "# aws dsql delete-cluster --identifier $PRIMARY_CLUSTER_ID --region us-east-1\n",
    "# \n",
    "# echo \"‚úÖ Cleanup initiated. Both clusters will be deleted shortly.\"\n",
    "\n",
    "echo \"üí° Uncomment the lines above to clean up your Aurora DSQL clusters\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "additional-resources",
   "metadata": {},
   "source": [
    "## Additional Resources üìö\n",
    "\n",
    "### Aurora Features\n",
    "- [Amazon Aurora PostgreSQL Limitless Database supports CloudWatch Database Insights](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/limitless-monitoring.cwdbi.html)\n",
    "- [Amazon Aurora PostgreSQL zero-ETL integrations support additional Regions](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.Aurora_Fea_Regions_DB-eng.Feature.Zero-ETL.html)\n",
    "- [Amazon Aurora PostgreSQL Limitless Database](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/limitless.html)\n",
    "\n",
    "### Serverless & Scaling\n",
    "- [End-of-life information for Amazon Aurora Serverless v1](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/[Aurora]-serverless-v2.upgrade.html#[Aurora]-serverless-v2.upgrade-from-serverless-v1-procedure)\n",
    "- [Amazon Aurora Serverless v2 supports automatic pause and resume](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/[Aurora]-serverless-v2-auto-pause.html)\n",
    "- [Amazon Aurora Serverless v2 supports 256 ACUs](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/[Aurora]-serverless-v2.how-it-works.html#[Aurora]-serverless-v2.how-it-works.capacity)\n",
    "\n",
    "### Security & Authentication\n",
    "- [Amazon RDS support for AWS Secrets Manager Integration]()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
