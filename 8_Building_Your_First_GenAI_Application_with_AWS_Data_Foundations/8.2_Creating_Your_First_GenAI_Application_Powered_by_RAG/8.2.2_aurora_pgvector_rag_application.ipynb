{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2.2 Building a RAG Application with Aurora PostgreSQL and pgvector\n",
    "\n",
    "<div style=\"background-color: #f8f9fa; border: 1px solid #e9ecef; border-radius: 8px; padding: 10px; margin: 10px;\">\n",
    "<strong>ðŸ“‹ Workshop Contents</strong>\n",
    "<ul style=\"line-height: 1.2;\">\n",
    "<li><a href=\"#Step-1-Install-Required-Libraries\">Step 1: Install Required Libraries</a></li>\n",
    "<li><a href=\"#Step-2-Connect-to-Aurora-PostgreSQL\">Step 2: Connect to Aurora PostgreSQL</a></li>\n",
    "<li><a href=\"#Step-3-Enable-pgvector-Extension-and-Create-Tables\">Step 3: Enable pgvector Extension and Create Tables</a></li>\n",
    "<li><a href=\"#Step-4-Create-HNSW-Index-for-Efficient-Vector-Search\">Step 4: Create HNSW Index for Efficient Vector Search</a></li>\n",
    "<li><a href=\"#Step-5-Create-Bedrock-Knowledge-Base-with-Web-Crawler\">Step 5: Create Bedrock Knowledge Base with Web Crawler</a></li>\n",
    "<li><a href=\"#Step-6-Query-the-Bedrock-Knowledge-Base\">Step 6: Query the Bedrock Knowledge Base</a></li>\n",
    "<li><a href=\"#Step-7-Create-Additional-Tables-for-Advanced-Queries\">Step 7: Create Additional Tables for Advanced Queries</a></li>\n",
    "<li><a href=\"#Step-8-Implement-Vector-Search-with-pgvector\">Step 8: Implement Vector Search with pgvector</a></li>\n",
    "<li><a href=\"#Step-9-Advanced-Query-Optimization-Techniques\">Step 9: Advanced Query Optimization Techniques</a></li>\n",
    "<li><a href=\"#Step-10-Complete-RAG-Pipeline-with-LLM-Response-Generation\">Step 10: Complete RAG Pipeline with LLM Response Generation</a></li>\n",
    "<li><a href=\"#Step-11-Compare-Both-RAG-Approaches\">Step 11: Compare Both RAG Approaches</a></li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "\n",
    "In this notebook, we'll build a complete [RAG (Retrieval Augmented Generation)](https://aws.amazon.com/what-is/retrieval-augmented-generation/) application using [Amazon Aurora PostgreSQL](https://aws.amazon.com/rds/aurora/postgresql-features/) with [pgvector](https://github.com/pgvector/pgvector) for vector storage and [Amazon Bedrock](https://aws.amazon.com/bedrock/) for embedding generation and LLM capabilities.\n",
    "\n",
    "RAG combines the power of retrieval-based systems with generative AI to produce more accurate, contextually relevant, and factual responses. By using Aurora PostgreSQL with pgvector, we can efficiently store and query vector embeddings, while Bedrock provides the foundation models for generating embeddings and responses.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to set up pgvector in Aurora PostgreSQL for vector similarity search\n",
    "- How to create and optimize vector indexes for performance\n",
    "- How to integrate with Amazon Bedrock for embeddings and LLM responses\n",
    "- How to implement advanced query optimization techniques for vector search\n",
    "\n",
    "## Prerequisites\n",
    "- AWS account with [Amazon Bedrock](https://aws.amazon.com/bedrock/) and [Aurora](https://aws.amazon.com/rds/aurora/) access\n",
    "- Aurora PostgreSQL cluster (you can use one created in [Section 2.1](../../2_Your_First_Database_on_AWS/2.1_Crearting_Your_First_Aurora_Cluster/README.MD) or create a new one)\n",
    "- Jupyter Notebook: You can launch a [free tier Amazon SageMaker Jupyter Notebook](../../1_Getting_Started_with_AWS/1.4_Setting_up_Your_Cookbook_Environment/README.MD)\n",
    "- Basic understanding of SQL and Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "We'll start by installing the necessary Python libraries for our RAG application:\n",
    "- [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html): AWS SDK for Python to interact with AWS services\n",
    "- [psycopg2](https://www.psycopg.org/docs/): PostgreSQL adapter for Python\n",
    "- [langchain](https://python.langchain.com/docs/get_started/introduction): Framework for developing applications powered by language models\n",
    "- [requests](https://requests.readthedocs.io/en/latest/): HTTP library for making API calls\n",
    "- [beautifulsoup4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/): Library for web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q boto3 psycopg2-binary langchain requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Connect to Aurora PostgreSQL\n",
    "\n",
    "Next, we'll establish a connection to our Aurora PostgreSQL database using credentials stored in [AWS Secrets Manager](https://aws.amazon.com/secrets-manager/). This approach follows security best practices by avoiding hardcoded credentials in our code.\n",
    "\n",
    "The process involves:\n",
    "1. Retrieving the secret containing database credentials\n",
    "2. Extracting connection parameters from the secret\n",
    "3. Establishing a connection to the Aurora PostgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# Initialize the AWS Secrets Manager client\n",
    "secrets_client = boto3.client('secretsmanager')\n",
    "\n",
    "# Specify the name of your Aurora PostgreSQL secret\n",
    "secret_name = \"aurora-postgresql-secret\"  # Replace with your actual secret name\n",
    "\n",
    "try:\n",
    "    # Retrieve the secret\n",
    "    response = secrets_client.get_secret_value(SecretId=secret_name)\n",
    "    secret = json.loads(response['SecretString'])\n",
    "    \n",
    "    # Extract database connection parameters from the secret\n",
    "    db_host = secret['host']\n",
    "    db_port = secret.get('port', 5432)\n",
    "    db_name = secret.get('dbname', 'postgres')\n",
    "    db_user = secret['username']\n",
    "    db_password = secret['password']\n",
    "    \n",
    "    print(f\"Successfully retrieved secret for {secret_name}\")\n",
    "    \n",
    "    # Connect to the database using the retrieved credentials\n",
    "    conn = psycopg2.connect(\n",
    "        host=db_host,\n",
    "        port=db_port,\n",
    "        database=db_name,\n",
    "        user=db_user,\n",
    "        password=db_password\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    print(\"Connected to the database successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving secret or connecting to database: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Enable pgvector Extension and Create Tables\n",
    "\n",
    "Now we'll set up [pgvector](https://github.com/pgvector/pgvector), an open-source PostgreSQL extension that enables vector similarity search. This extension allows us to store embeddings as a native data type and perform efficient similarity searches.\n",
    "\n",
    "We'll perform the following tasks:\n",
    "1. Enable the pgvector extension in our database\n",
    "2. Verify the installed pgvector version\n",
    "3. Create a table for storing documents and their vector embeddings\n",
    "\n",
    "The `documents` table will include a `VECTOR(1536)` column to store embeddings from the Titan Embeddings model, which produces 1536-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable pgvector extension\n",
    "cursor.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "\n",
    "# Check pgvector version\n",
    "cursor.execute(\"SELECT extversion FROM pg_extension WHERE extname = 'vector';\")\n",
    "pgvector_version = cursor.fetchone()[0]\n",
    "print(f\"pgvector version: {pgvector_version}\")\n",
    "\n",
    "# Create a table for documents and their embeddings\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS documents (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    title TEXT NOT NULL,\n",
    "    content TEXT NOT NULL,\n",
    "    url TEXT,\n",
    "    category TEXT,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    embedding VECTOR(1536)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"Database schema created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create HNSW Index for Efficient Vector Search\n",
    "\n",
    "[HNSW (Hierarchical Navigable Small World)](https://arxiv.org/abs/1603.09320) indexes provide faster query performance compared to traditional [IVF (Inverted File)](https://en.wikipedia.org/wiki/Inverted_index) indexes, especially for high-dimensional vectors. HNSW is an approximate nearest neighbor search algorithm that creates a multi-layered graph structure for efficient navigation.\n",
    "\n",
    "Key HNSW parameters we're configuring:\n",
    "- `m = 16`: Controls the maximum number of connections per node in the graph\n",
    "- `ef_construction = 64`: Controls the size of the dynamic candidate list during index construction\n",
    "\n",
    "These settings balance index build time, search performance, and memory usage. Higher values generally provide better search quality at the cost of increased memory usage and build time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an HNSW index for efficient vector search\n",
    "cursor.execute(\"\"\"\n",
    "CREATE INDEX IF NOT EXISTS documents_embedding_hnsw_idx \n",
    "ON documents \n",
    "USING hnsw (embedding vector_cosine_ops)\n",
    "WITH (m = 16, ef_construction = 64);\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"HNSW index created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Bedrock Knowledge Base with Web Crawler\n",
    "\n",
    "In this step, we'll create an [Amazon Bedrock Knowledge Base](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html) that uses our Aurora PostgreSQL database as its vector store. A Knowledge Base in Bedrock provides a managed solution for implementing RAG applications.\n",
    "\n",
    "We'll perform the following tasks:\n",
    "1. Create an IAM role with necessary permissions for Bedrock\n",
    "2. Configure the Knowledge Base to use Aurora PostgreSQL as the vector store\n",
    "3. Set up a web crawler data source to ingest AWS documentation\n",
    "4. Monitor the ingestion process\n",
    "\n",
    "Using the [web crawler](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-create-datasource.html#knowledge-base-create-datasource-web) capability, we can automatically extract, process, and index content from the Aurora PostgreSQL documentation website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "# Initialize Bedrock clients\n",
    "bedrock = boto3.client('bedrock')\n",
    "agents = boto3.client('bedrock-agent')\n",
    "\n",
    "# Define knowledge base parameters\n",
    "kb_name = \"aurora-postgresql-kb\"\n",
    "kb_description = \"Knowledge base for Aurora PostgreSQL documentation\"\n",
    "embedding_model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "# Create a unique service role name\n",
    "role_name = f\"bedrock-kb-role-{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "# Create IAM role for Bedrock Knowledge Base\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "# Create the IAM role with trust policy\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": \"bedrock.amazonaws.com\"},\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    role_response = iam.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description=\"Role for Bedrock Knowledge Base\",\n",
    "        Tags=[\n",
    "            {\n",
    "                'Key': 'CreationSource',\n",
    "                'Value': 'aws-database-cookbook-v2025.8'\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Attach necessary policies\n",
    "    iam.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn=\"arn:aws:iam::aws:policy/AmazonBedrockFullAccess\"\n",
    "    )\n",
    "    \n",
    "    # Wait for role to propagate\n",
    "    print(\"Waiting for IAM role to propagate...\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "    role_arn = role_response['Role']['Arn']\n",
    "    print(f\"Created role: {role_arn}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating role: {e}\")\n",
    "    # If role already exists, get its ARN\n",
    "    role_arn = iam.get_role(RoleName=role_name)['Role']['Arn']\n",
    "    print(f\"Using existing role: {role_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the knowledge base with Aurora PostgreSQL as vector store\n",
    "try:\n",
    "    # Get Aurora PostgreSQL connection details\n",
    "    rds_data_config = {\n",
    "        \"databaseConnectionConfiguration\": {\n",
    "            \"rdsConfiguration\": {\n",
    "                \"host\": db_host,\n",
    "                \"port\": db_port,\n",
    "                \"databaseName\": db_name,\n",
    "                \"credentials\": {\n",
    "                    \"secretArn\": f\"arn:aws:secretsmanager:{boto3.session.Session().region_name}:{boto3.client('sts').get_caller_identity()['Account']}:secret:{secret_name}\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create knowledge base with Aurora PostgreSQL as vector store\n",
    "    kb_response = agents.create_knowledge_base(\n",
    "        name=kb_name,\n",
    "        description=kb_description,\n",
    "        roleArn=role_arn,\n",
    "        knowledgeBaseConfiguration={\n",
    "            \"type\": \"VECTOR\",\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": f\"arn:aws:bedrock:{boto3.session.Session().region_name}::foundation-model/{embedding_model_id}\"\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration={\n",
    "            \"type\": \"RDS\",\n",
    "            \"rdsStorageConfiguration\": rds_data_config\n",
    "        },\n",
    "        tags={\n",
    "            'CreationSource': 'aws-database-cookbook-v2025.8'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    kb_id = kb_response['knowledgeBase']['knowledgeBaseId']\n",
    "    print(f\"Created knowledge base with ID: {kb_id}\")\n",
    "    \n",
    "    # Wait for knowledge base to be active\n",
    "    print(\"Waiting for knowledge base to become active...\")\n",
    "    while True:\n",
    "        kb_status = agents.get_knowledge_base(knowledgeBaseId=kb_id)['knowledgeBase']['status']\n",
    "        print(f\"Knowledge base status: {kb_status}\")\n",
    "        if kb_status == \"ACTIVE\":\n",
    "            break\n",
    "        time.sleep(10)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating knowledge base: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add web crawler data source\n",
    "try:\n",
    "    # URL of Aurora PostgreSQL documentation\n",
    "    url = \"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraPostgreSQL.html\"\n",
    "    \n",
    "    # Create data source with web crawler\n",
    "    data_source_response = agents.create_data_source(\n",
    "        knowledgeBaseId=kb_id,\n",
    "        name=\"aurora-postgresql-docs\",\n",
    "        description=\"Aurora PostgreSQL documentation from AWS docs\",\n",
    "        dataSourceConfiguration={\n",
    "            \"type\": \"WEB\",\n",
    "            \"webConfiguration\": {\n",
    "                \"url\": url,\n",
    "                \"crawlerConfiguration\": {\n",
    "                    \"crawlMode\": \"SUBPAGES\",\n",
    "                    \"crawlDepth\": 2,  # Crawl up to 2 levels deep\n",
    "                    \"maxUrls\": 20,    # Limit to 20 URLs for demo purposes\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        vectorIngestionConfiguration={\n",
    "            \"chunkingConfiguration\": {\n",
    "                \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "                \"fixedSizeChunkingConfiguration\": {\n",
    "                    \"maxTokens\": 300,\n",
    "                    \"overlapPercentage\": 20\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    data_source_id = data_source_response['dataSource']['dataSourceId']\n",
    "    print(f\"Created data source with ID: {data_source_id}\")\n",
    "    \n",
    "    # Start data source ingestion\n",
    "    ingestion_job = agents.start_ingestion_job(\n",
    "        knowledgeBaseId=kb_id,\n",
    "        dataSourceId=data_source_id\n",
    "    )\n",
    "    \n",
    "    ingestion_job_id = ingestion_job['ingestionJob']['ingestionJobId']\n",
    "    print(f\"Started ingestion job with ID: {ingestion_job_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating data source or starting ingestion: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor ingestion job status\n",
    "def check_ingestion_status(kb_id, data_source_id, ingestion_job_id):\n",
    "    try:\n",
    "        response = agents.get_ingestion_job(\n",
    "            knowledgeBaseId=kb_id,\n",
    "            dataSourceId=data_source_id,\n",
    "            ingestionJobId=ingestion_job_id\n",
    "        )\n",
    "        \n",
    "        status = response['ingestionJob']['status']\n",
    "        metrics = response['ingestionJob'].get('metrics', {})\n",
    "        \n",
    "        print(f\"Ingestion status: {status}\")\n",
    "        if metrics:\n",
    "            print(f\"Documents processed: {metrics.get('documentsProcessed', 'N/A')}\")\n",
    "            print(f\"Documents failed: {metrics.get('documentsFailed', 'N/A')}\")\n",
    "            print(f\"Vectors ingested: {metrics.get('vectorsIngested', 'N/A')}\")\n",
    "        \n",
    "        return status\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking ingestion status: {e}\")\n",
    "        return None\n",
    "\n",
    "# Check status every 30 seconds for up to 10 minutes\n",
    "for _ in range(20):\n",
    "    status = check_ingestion_status(kb_id, data_source_id, ingestion_job_id)\n",
    "    if status in [\"COMPLETE\", \"FAILED\"]:\n",
    "        break\n",
    "    print(\"Waiting 30 seconds...\")\n",
    "    time.sleep(30)\n",
    "\n",
    "print(\"Ingestion process completed or timed out.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Additional Tables for Advanced Queries\n",
    "\n",
    "To demonstrate the power of combining vector search with traditional relational database capabilities, we'll create additional tables for metadata and tagging. This [relational model](https://en.wikipedia.org/wiki/Relational_model) allows us to perform more sophisticated queries that combine semantic similarity with structured filtering.\n",
    "\n",
    "We'll create:\n",
    "1. A `document_metadata` table to store additional information about documents (author, publication date, version, etc.)\n",
    "2. A `document_tags` table to implement a [many-to-many relationship](https://en.wikipedia.org/wiki/Many-to-many_(data_model)) between documents and tags\n",
    "3. Appropriate indexes to optimize query performance\n",
    "\n",
    "This approach demonstrates how to leverage both the vector capabilities of pgvector and the relational capabilities of PostgreSQL in a single application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Initialize Bedrock client for embeddings\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1'  # Change to your region\n",
    ")\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Generate embedding using Amazon Titan Embeddings v2 model\"\"\"\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId='amazon.titan-embed-text-v2:0',  # Using v2 model as specified\n",
    "        contentType='application/json',\n",
    "        accept='application/json',\n",
    "        body=json.dumps({\n",
    "            \"inputText\": text\n",
    "        })\n",
    "    )\n",
    "    response_body = json.loads(response['body'].read())\n",
    "    return response_body['embedding']\n",
    "\n",
    "# Create metadata table for additional document information\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS document_metadata (\n",
    "    document_id INTEGER PRIMARY KEY REFERENCES documents(id),\n",
    "    author TEXT,\n",
    "    publication_date DATE,\n",
    "    version TEXT,\n",
    "    importance_score FLOAT,\n",
    "    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Create table for document tags\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS document_tags (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    document_id INTEGER REFERENCES documents(id),\n",
    "    tag TEXT NOT NULL,\n",
    "    UNIQUE(document_id, tag)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Create index on document_tags for faster lookups\n",
    "cursor.execute(\"\"\"\n",
    "CREATE INDEX IF NOT EXISTS document_tags_tag_idx ON document_tags(tag);\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"Additional tables created successfully!\")\n",
    "\n",
    "# Add sample metadata and tags for existing documents\n",
    "cursor.execute(\"SELECT id, title FROM documents LIMIT 10\")\n",
    "existing_docs = cursor.fetchall()\n",
    "\n",
    "for doc_id, title in existing_docs:\n",
    "    # Add metadata\n",
    "    cursor.execute(\n",
    "        \"\"\"INSERT INTO document_metadata \n",
    "           (document_id, author, publication_date, version, importance_score) \n",
    "           VALUES (%s, %s, %s, %s, %s)\n",
    "           ON CONFLICT (document_id) DO NOTHING\"\"\",\n",
    "        (doc_id, \"AWS Documentation Team\", \"2023-01-01\", \"1.0\", 0.8)\n",
    "    )\n",
    "    \n",
    "    # Add tags based on title\n",
    "    tags = [\"Aurora\", \"PostgreSQL\"]\n",
    "    if \"performance\" in title.lower():\n",
    "        tags.append(\"Performance\")\n",
    "    if \"security\" in title.lower():\n",
    "        tags.append(\"Security\")\n",
    "    if \"feature\" in title.lower() or \"features\" in title.lower():\n",
    "        tags.append(\"Features\")\n",
    "    \n",
    "    for tag in tags:\n",
    "        cursor.execute(\n",
    "            \"\"\"INSERT INTO document_tags (document_id, tag) \n",
    "               VALUES (%s, %s) \n",
    "               ON CONFLICT (document_id, tag) DO NOTHING\"\"\",\n",
    "            (doc_id, tag)\n",
    "        )\n",
    "\n",
    "conn.commit()\n",
    "print(\"Added metadata and tags to existing documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Query the Bedrock Knowledge Base\n",
    "\n",
    "Now let's create functions to query our Bedrock Knowledge Base and generate responses using the LLM. This demonstrates the [RAG pattern](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html) where we:\n",
    "\n",
    "1. Retrieve relevant information from the Knowledge Base based on a user query\n",
    "2. Augment the LLM prompt with this retrieved context\n",
    "3. Generate a response that's grounded in the retrieved information\n",
    "\n",
    "We'll use the [Bedrock Agent Runtime API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Amazon_Bedrock_Agent_Runtime.html) to query the Knowledge Base and the [Bedrock Runtime API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Amazon_Bedrock_Runtime.html) to generate responses using Claude or another foundation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query the knowledge base\n",
    "def query_knowledge_base(kb_id, query_text, max_results=3):\n",
    "    try:\n",
    "        # Initialize the Bedrock Agent Runtime client\n",
    "        bedrock_agent_runtime = boto3.client('bedrock-agent-runtime')\n",
    "        \n",
    "        # Query the knowledge base\n",
    "        response = bedrock_agent_runtime.retrieve(\n",
    "            knowledgeBaseId=kb_id,\n",
    "            retrievalQuery={\n",
    "                'text': query_text\n",
    "            },\n",
    "            retrievalConfiguration={\n",
    "                'vectorSearchConfiguration': {\n",
    "                    'numberOfResults': max_results\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return response['retrievalResults']\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying knowledge base: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to generate RAG response using Bedrock model\n",
    "def generate_kb_response(kb_id, query_text, model_id=\"anthropic.claude-v2\"):\n",
    "    # Step 1: Retrieve relevant chunks from knowledge base\n",
    "    results = query_knowledge_base(kb_id, query_text)\n",
    "    \n",
    "    if not results:\n",
    "        return \"Unable to retrieve information from the knowledge base.\"\n",
    "    \n",
    "    # Step 2: Build context from retrieved chunks\n",
    "    context = \"\\n\\n\".join([result['content']['text'] for result in results])\n",
    "    \n",
    "    # Step 3: Generate response using Bedrock model\n",
    "    bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Human: Answer the following question based on the provided context.\n",
    "    If you cannot answer the question based on the context, say \"I don't have enough information to answer this question.\"\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {query_text}\n",
    "    \n",
    "    Assistant:\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=model_id,\n",
    "            contentType='application/json',\n",
    "            accept='application/json',\n",
    "            body=json.dumps({\n",
    "                \"prompt\": prompt,\n",
    "                \"max_tokens_to_sample\": 500,\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.9,\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response['body'].read())\n",
    "        return response_body['completion']\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return \"Error generating response from the model.\"\n",
    "\n",
    "# Test the RAG pipeline with our knowledge base\n",
    "query = \"What are the key features of Aurora PostgreSQL?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Replace with your actual knowledge base ID\n",
    "response = generate_kb_response(kb_id, query)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Implement Vector Search with pgvector\n",
    "\n",
    "Now we'll implement a custom vector search function using pgvector's capabilities. This demonstrates how to perform [vector similarity search](https://www.pinecone.io/learn/vector-similarity/) directly in PostgreSQL without relying on external vector databases.\n",
    "\n",
    "The process involves:\n",
    "1. Converting a text query into a vector embedding using Bedrock's embedding model\n",
    "2. Using pgvector's `<=>` operator to calculate [cosine distance](https://en.wikipedia.org/wiki/Cosine_similarity) between the query embedding and document embeddings\n",
    "3. Retrieving the most similar documents based on this distance metric\n",
    "\n",
    "The `1 - (embedding <=> %s)` expression converts cosine distance to cosine similarity, which ranges from 0 (completely dissimilar) to 1 (identical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(query_text, top_k=3):\n",
    "    \"\"\"Perform vector similarity search\"\"\"\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = get_embedding(query_text)\n",
    "    \n",
    "    # Perform vector similarity search\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT title, content, url, 1 - (embedding <=> %s) as similarity\n",
    "    FROM documents\n",
    "    ORDER BY embedding <=> %s\n",
    "    LIMIT %s\n",
    "    \"\"\", (query_embedding, query_embedding, top_k))\n",
    "    \n",
    "    results = cursor.fetchall()\n",
    "    return results\n",
    "\n",
    "# Test the search\n",
    "query = \"What are the key features of Aurora PostgreSQL?\"\n",
    "search_results = vector_search(query)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top results:\")\n",
    "for title, content, url, similarity in search_results:\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Similarity: {similarity:.4f}\")\n",
    "    print(f\"URL: {url}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Advanced Query Optimization Techniques\n",
    "\n",
    "Now let's explore some advanced [query optimization techniques](https://aws.amazon.com/blogs/database/optimize-query-performance-with-pgvector-in-amazon-aurora-postgresql/) for pgvector. These techniques can significantly improve search performance, especially for large datasets or complex queries.\n",
    "\n",
    "We'll cover:\n",
    "- Iterative index scans for filtered queries\n",
    "- Creating proper indexes for filtering conditions\n",
    "- Partial indexing and partitioning for specialized workloads\n",
    "\n",
    "These optimizations are particularly important for production RAG applications where response time is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Iterative Index Scans (pgvector 0.8.0+)\n",
    "\n",
    "[Iterative index scans](https://github.com/pgvector/pgvector/blob/master/README.md#query-options) automatically scan more of the index until enough results are found, which is particularly useful for filtered queries. This feature was introduced in pgvector 0.8.0 and provides a significant performance improvement for queries that combine vector similarity with traditional SQL filters.\n",
    "\n",
    "The `ef_search` parameter controls how many candidates are considered during the search. Higher values increase accuracy but decrease performance. We'll demonstrate this technique by implementing an advanced search function that combines vector similarity with metadata filtering and joins across multiple tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of iterative index scan with filtering and joins\n",
    "def advanced_vector_search(query_text, tag=None, min_importance=0.5, top_k=3):\n",
    "    \"\"\"Perform filtered vector similarity search with iterative scanning and joins\"\"\"\n",
    "    query_embedding = get_embedding(query_text)\n",
    "    \n",
    "    # Set ef_search parameter for iterative scanning\n",
    "    cursor.execute(\"SET hnsw.ef_search = 100;\")\n",
    "    \n",
    "    # Build the query based on filters\n",
    "    query = \"\"\"\n",
    "    SELECT d.title, d.content, d.url, 1 - (d.embedding <=> %s) as similarity,\n",
    "           m.author, m.importance_score, array_agg(t.tag) as tags\n",
    "    FROM documents d\n",
    "    JOIN document_metadata m ON d.id = m.document_id\n",
    "    LEFT JOIN document_tags t ON d.id = t.document_id\n",
    "    WHERE m.importance_score >= %s\n",
    "    \"\"\"\n",
    "    \n",
    "    params = [query_embedding, min_importance]\n",
    "    \n",
    "    # Add tag filter if provided\n",
    "    if tag:\n",
    "        query += \"AND EXISTS (SELECT 1 FROM document_tags WHERE document_id = d.id AND tag = %s)\"\n",
    "        params.append(tag)\n",
    "    \n",
    "    # Complete the query\n",
    "    query += \"\"\"\n",
    "    GROUP BY d.id, d.title, d.content, d.url, d.embedding, m.author, m.importance_score\n",
    "    ORDER BY d.embedding <=> %s\n",
    "    LIMIT %s\n",
    "    \"\"\"\n",
    "    \n",
    "    params.extend([query_embedding, top_k])\n",
    "    \n",
    "    # Execute the query\n",
    "    cursor.execute(query, params)\n",
    "    results = cursor.fetchall()\n",
    "    return results\n",
    "\n",
    "# Test advanced search with joins\n",
    "advanced_results = advanced_vector_search(\n",
    "    \"What are the key features of Aurora PostgreSQL?\", \n",
    "    tag=\"Features\", \n",
    "    min_importance=0.7\n",
    ")\n",
    "\n",
    "print(\"Advanced search results with joins:\")\n",
    "for title, content, url, similarity, author, importance, tags in advanced_results:\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Author: {author}\")\n",
    "    print(f\"Importance: {importance:.2f}\")\n",
    "    print(f\"Tags: {', '.join(tags)}\")\n",
    "    print(f\"Similarity: {similarity:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Creating Proper Indexes for Filtering\n",
    "\n",
    "When combining vector search with filtering, it's important to create appropriate [indexes](https://www.postgresql.org/docs/current/indexes.html) on the filter columns. PostgreSQL offers several index types, each optimized for different query patterns:\n",
    "\n",
    "- [B-tree indexes](https://www.postgresql.org/docs/current/btree-intro.html): The default index type, good for equality and range queries\n",
    "- [GIN indexes](https://www.postgresql.org/docs/current/gin-intro.html): Generalized Inverted Indexes, excellent for full-text search\n",
    "- [BRIN indexes](https://www.postgresql.org/docs/current/brin-intro.html): Block Range INdexes, efficient for large tables with naturally ordered data\n",
    "\n",
    "We'll create these different index types to optimize various filtering scenarios in our RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create B-tree index on category column\n",
    "cursor.execute(\"\"\"\n",
    "CREATE INDEX IF NOT EXISTS documents_category_idx \n",
    "ON documents (category);\n",
    "\"\"\")\n",
    "\n",
    "# Create GIN index on title for full-text search\n",
    "cursor.execute(\"\"\"\n",
    "CREATE INDEX IF NOT EXISTS documents_title_idx \n",
    "ON documents USING gin(to_tsvector('english', title));\n",
    "\"\"\")\n",
    "\n",
    "# Create BRIN index on created_at for range queries\n",
    "cursor.execute(\"\"\"\n",
    "CREATE INDEX IF NOT EXISTS documents_created_at_idx \n",
    "ON documents USING brin(created_at);\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"Additional indexes created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Partial Indexing and Partitioning\n",
    "\n",
    "For better performance with specific filtering patterns, we can use [partial indexes](https://www.postgresql.org/docs/current/indexes-partial.html) or [table partitioning](https://www.postgresql.org/docs/current/ddl-partitioning.html). These advanced PostgreSQL features can significantly improve query performance for large datasets:\n",
    "\n",
    "- **Partial Indexes**: Create indexes that only include rows matching a specific condition, reducing index size and maintenance overhead\n",
    "- **Table Partitioning**: Divide large tables into smaller, more manageable pieces based on specific criteria (e.g., category)\n",
    "\n",
    "We'll demonstrate both techniques using our document collection, creating a partial index for Aurora PostgreSQL-specific documents and implementing list partitioning by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a partial index for a specific category\n",
    "cursor.execute(\"\"\"\n",
    "CREATE INDEX IF NOT EXISTS documents_aurora_idx \n",
    "ON documents (id) \n",
    "WHERE category = 'Aurora PostgreSQL';\n",
    "\"\"\")\n",
    "\n",
    "# Example of creating a partitioned table (for demonstration only)\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS documents_partitioned (\n",
    "    id SERIAL,\n",
    "    title TEXT NOT NULL,\n",
    "    content TEXT NOT NULL,\n",
    "    url TEXT,\n",
    "    category TEXT,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    embedding VECTOR(1536),\n",
    "    PRIMARY KEY (category, id)\n",
    ") PARTITION BY LIST (category);\n",
    "\"\"\")\n",
    "\n",
    "# Create a partition for Aurora PostgreSQL documents\n",
    "try:\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE documents_aurora PARTITION OF documents_partitioned\n",
    "    FOR VALUES IN ('Aurora PostgreSQL');\n",
    "    \"\"\")\n",
    "    print(\"Partition created successfully!\")\n",
    "except psycopg2.errors.DuplicateTable:\n",
    "    print(\"Partition already exists.\")\n",
    "    conn.rollback()\n",
    "else:\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Complete RAG Pipeline with LLM Response Generation\n",
    "\n",
    "Now we'll implement a complete [RAG pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html) using our custom pgvector implementation. This pipeline combines vector search with LLM response generation to create a system that can answer questions based on our document collection.\n",
    "\n",
    "The pipeline consists of three main steps:\n",
    "1. **Retrieval**: Find relevant documents using vector similarity search\n",
    "2. **Augmentation**: Enhance the prompt with retrieved context\n",
    "3. **Generation**: Generate a response using a foundation model from [Amazon Bedrock](https://aws.amazon.com/bedrock/)\n",
    "\n",
    "This approach helps ground the LLM's responses in factual information from our document collection, reducing hallucinations and improving accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pgvector_response(query):\n",
    "    \"\"\"Complete RAG pipeline with pgvector search and LLM response generation\"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    search_results = vector_search(query, top_k=3)\n",
    "    \n",
    "    if not search_results:\n",
    "        return \"No relevant documents found in the database.\"\n",
    "    \n",
    "    # Step 2: Build context from retrieved documents\n",
    "    context = \"\\n\\n\".join([content for _, content, _, _ in search_results])\n",
    "    \n",
    "    # Step 3: Generate response using Amazon Bedrock\n",
    "    prompt = f\"\"\"\n",
    "    Human: Answer the following question based on the provided context.\n",
    "    If you cannot answer the question based on the context, say \"I don't have enough information to answer this question.\"\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {query}\n",
    "    \n",
    "    Assistant:\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId='anthropic.claude-v2',  # Or your preferred model\n",
    "            contentType='application/json',\n",
    "            accept='application/json',\n",
    "            body=json.dumps({\n",
    "                \"prompt\": prompt,\n",
    "                \"max_tokens_to_sample\": 500,\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.9,\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response['body'].read())\n",
    "        return response_body['completion']\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return \"Error generating response from the model.\"\n",
    "\n",
    "# Test the complete pgvector RAG pipeline\n",
    "query = \"What are the benefits of using Aurora PostgreSQL?\"\n",
    "response = generate_pgvector_response(query)\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Compare Both RAG Approaches\n",
    "\n",
    "Let's compare the results from both our RAG implementations - the custom pgvector approach and the [Bedrock Knowledge Base](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html) approach. This comparison will help us understand the trade-offs between:\n",
    "\n",
    "1. **Custom Implementation**: More control and flexibility, but requires more development effort\n",
    "2. **Managed Service**: Easier to set up and maintain, but potentially less customizable\n",
    "\n",
    "We'll use the same query for both approaches and compare the quality, relevance, and accuracy of the responses. This will help you decide which approach is better suited for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both RAG approaches with the same query\n",
    "test_query = \"What are the key features and benefits of Aurora PostgreSQL?\"\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(\"=== pgvector RAG Response ===\")\n",
    "pgvector_response = generate_pgvector_response(test_query)\n",
    "print(pgvector_response)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"=== Bedrock Knowledge Base RAG Response ===\")\n",
    "kb_response = generate_kb_response(kb_id, test_query)\n",
    "print(kb_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've built a complete [RAG application](https://aws.amazon.com/what-is/retrieval-augmented-generation/) using [Amazon Aurora PostgreSQL](https://aws.amazon.com/rds/aurora/postgresql-features/) with [pgvector](https://github.com/pgvector/pgvector) and [Amazon Bedrock](https://aws.amazon.com/bedrock/). We've covered:\n",
    "\n",
    "1. Setting up pgvector in Aurora PostgreSQL for vector similarity search\n",
    "2. Creating and optimizing [vector indexes](https://github.com/pgvector/pgvector#indexing) (HNSW) for performance\n",
    "3. Creating a [Bedrock Knowledge Base](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html) with web crawler for document ingestion\n",
    "4. Implementing [vector search](https://www.pinecone.io/learn/vector-similarity/) for semantic document retrieval\n",
    "5. Advanced query optimization techniques:\n",
    "   - [Iterative index scans](https://github.com/pgvector/pgvector/blob/master/README.md#query-options) for filtered queries\n",
    "   - [Proper indexing](https://www.postgresql.org/docs/current/indexes.html) for filtering conditions\n",
    "   - [Partial indexing](https://www.postgresql.org/docs/current/indexes-partial.html) and [partitioning](https://www.postgresql.org/docs/current/ddl-partitioning.html) for specialized workloads\n",
    "6. Building complete RAG pipelines with both approaches:\n",
    "   - Custom pgvector implementation for maximum flexibility\n",
    "   - Managed Bedrock Knowledge Base for ease of use\n",
    "\n",
    "These techniques provide a solid foundation for building production-ready RAG applications with optimal performance and cost efficiency. By leveraging the power of Aurora PostgreSQL's relational capabilities combined with pgvector's vector search functionality, you can create sophisticated AI applications that deliver accurate, contextually relevant responses.\n",
    "\n",
    "For more information, check out these resources:\n",
    "- [Amazon Aurora PostgreSQL Documentation](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraPostgreSQL.html)\n",
    "- [pgvector GitHub Repository](https://github.com/pgvector/pgvector)\n",
    "- [Amazon Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)\n",
    "- [AWS RAG Reference Implementation](https://github.com/aws-samples/rag-with-amazon-bedrock)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
